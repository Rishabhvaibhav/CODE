{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0bb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c586ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362092fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e316a05",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17768\\2346746226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Airline_DataFrame\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.ui.port'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'4090'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Airline_DataFrame\").config('spark.ui.port','4090').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6a75a",
   "metadata": {},
   "source": [
    "# Create a dataframe flight.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dataframe = spark.read.csv('flights.csv',header = True)\n",
    "Airline_dataframe.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Airline_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7a612",
   "metadata": {},
   "source": [
    "# Check all the Null value of data in each Column in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\"\"\"\n",
    "In PySpark, you can use the .isNull() method to check for NULL values in a Airline_dataframe and\n",
    "the .sum() method to count the number of NULL values in each column. \n",
    "You can chain these methods together to check for NULL values in each column of a Airline_dataframe\n",
    "issue : not understand *\n",
    "\"\"\"\n",
    "\n",
    "# Check for NULL values in each column\n",
    "Airline_dataframe_null_counts = Airline_dataframe.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in Airline_dataframe.columns))\n",
    "\n",
    "Airline_dataframe_null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8419b",
   "metadata": {},
   "source": [
    "# Typecast the datatype of column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dataframe = Airline_dataframe \\\n",
    "    .withColumn('YEAR', Airline_dataframe['YEAR'].cast('date')) \\\n",
    "    .withColumn('MONTH', Airline_dataframe['MONTH'].cast('date')) \\\n",
    "    .withColumn('DAY', Airline_dataframe['DAY'].cast('date')) \\\n",
    "    .withColumn('AIRLINE', Airline_dataframe['AIRLINE'].cast('int')) \\\n",
    "    .withColumn('FLIGHT_NUMBER', Airline_dataframe['FLIGHT_NUMBER'].cast('int')) \\\n",
    "    .withColumn('TAIL_NUMBER', Airline_dataframe['TAIL_NUMBER'].cast('int')) \\\n",
    "    .withColumn('SCHEDULED_DEPARTURE', Airline_dataframe['SCHEDULED_DEPARTURE'].cast('int')) \\\n",
    "    .withColumn('DEPARTURE_TIME', Airline_dataframe['DEPARTURE_TIME'].cast('int')) \\\n",
    "    .withColumn('DEPARTURE_DELAY', Airline_dataframe['DEPARTURE_DELAY'].cast('int')) \\\n",
    "    .withColumn('TAXI_OUT', Airline_dataframe['TAXI_OUT'].cast('int')) \\\n",
    "    .withColumn('WHEELS_OFF', Airline_dataframe['WHEELS_OFF'].cast('int')) \\\n",
    "    .withColumn('SCHEDULED_TIME', Airline_dataframe['SCHEDULED_TIME'].cast('int'))\\\n",
    "    .withColumn('ELAPSED_TIME', Airline_dataframe['ELAPSED_TIME'].cast('int')) \\\n",
    "    .withColumn('AIR_TIME', Airline_dataframe['AIR_TIME'].cast('int')) \\\n",
    "    .withColumn('DISTANCE', Airline_dataframe['DISTANCE'].cast('int')) \\\n",
    "    .withColumn('WHEELS_ON', Airline_dataframe['WHEELS_ON'].cast('int')) \\\n",
    "    .withColumn('TAXI_IN', Airline_dataframe['TAXI_IN'].cast('int')) \\\n",
    "    .withColumn('SCHEDULED_ARRIVAL', Airline_dataframe['SCHEDULED_ARRIVAL'].cast('int')) \\\n",
    "    .withColumn('ARRIVAL_TIME', Airline_dataframe['ARRIVAL_TIME'].cast('int')) \\\n",
    "    .withColumn('DIVERTED', Airline_dataframe['DIVERTED'].cast('int')) \\\n",
    "    .withColumn('CANCELLED', Airline_dataframe['CANCELLED'].cast('int')) \\\n",
    "    .withColumn('AIR_SYSTEM_DELAY', Airline_dataframe['AIR_SYSTEM_DELAY'].cast('int')) \\\n",
    "    .withColumn('SECURITY_DELAY', Airline_dataframe['SECURITY_DELAY'].cast('int')) \\\n",
    "    .withColumn('AIRLINE_DELAY', Airline_dataframe['AIRLINE_DELAY'].cast('int')) \\\n",
    "    .withColumn('LATE_AIRCRAFT_DELAY', Airline_dataframe['LATE_AIRCRAFT_DELAY'].cast('int')) \\\n",
    "    .withColumn('WEATHER_DELAY', Airline_dataframe['WEATHER_DELAY'].cast('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef7575",
   "metadata": {},
   "source": [
    "# Create a new column :- MM:DD:YYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0af371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "Airline_dataframe = Airline_dataframe.withColumn(\"MM:DD:YYYY\", concat_ws(':', Airline_dataframe[\"MONTH\"], Airline_dataframe[\"DAY\"], Airline_dataframe[\"YEAR\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a172e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Airline_dataframe.select(\"MM:DD:YYYY\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "New col in that -ve => earlier depature\n",
    "                  0 => on time\n",
    "\t\t\t    +ve  => delay in depature\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc323d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "Airline_dataframe = Airline_dataframe.withColumn(\"DEPARTURE_DELAY_STATUS\", when(Airline_dataframe[\"DEPARTURE_DELAY\"] < 0, \"BEFORE\")\n",
    "                      .when(Airline_dataframe[\"DEPARTURE_DELAY\"] > 0, \"AFTER\")\n",
    "                      .otherwise(\"ON_TIME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Airline_dataframe.select(\"DEPARTURE_DELAY_STATUS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd792b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#CANCELLATION_REASON' indicates with a letter the reason for the cancellation of the flight.\n",
    "#A - Carrier; B - Weather; C - National Air System; D - Security\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Assuming a DataFrame named \"flights\" with a column named \"reason\"\n",
    "Airline_dataframe = Airline_dataframe.withColumn(\"CANCELLATION_ISSUE\", when(Airline_dataframe[\"CANCELLATION_REASON\"] == \"Carrier\", \"A\").otherwise(\n",
    "    when(Airline_dataframe[\"CANCELLATION_REASON\"] == \"Weather\", \"B\").otherwise(\n",
    "         when(Airline_dataframe[\"CANCELLATION_REASON\"] == \"National Air System\", \"C\").otherwise(\n",
    "             when(Airline_dataframe[\"CANCELLATION_REASON\"] == \"Security\", \"D\").otherwise(\"NA\")\n",
    "         )\n",
    "    )\n",
    ")\n",
    "                                                )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ce315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Airline_dataframe.select(\"CANCELLATION_ISSUE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94718a67",
   "metadata": {},
   "source": [
    "# replace the with original values with state\n",
    "origin_airport  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519daabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airport_dataframe = spark.read.csv('airports.csv',header = True)\n",
    "Airport_dataframe.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge_data_airline_with_airport =Airline_dataframe.join(Airport_dataframe,Airline_dataframe[\"ORIGIN_AIRPORT\"] == Airport_dataframe[\"IATA_CODE\"],\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove_column = Merge_data_airline_with_airport.drop(\"CITY\",\"STATE\",\"COUNTRY\",\"LATITUDE\",\"LONGITUDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_value = Remove_column.withColumnRenamed(\"AIRPORT\",\"ORIGIN_AIRPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_value.select('IATA_CODE',\"ORIGIN_AIRPORT\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6250f",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "destination_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7228a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge_data =Airline_dataframe.join(Airport_dataframe,Airline_dataframe[\"DESTINATION_AIRPORT\"] == Airport_dataframe[\"IATA_CODE\"],\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea81005",
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove_columns = Merge_data_airline_with_airport.drop(\"CITY\",\"STATE\",\"COUNTRY\",\"LATITUDE\",\"LONGITUDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba60d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = Remove_columns.withColumnRenamed(\"AIRPORT\",\"DESTINATION_AIRPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfda1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.select('IATA_CODE',\"DESTINATION_AIRPORT\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299daac",
   "metadata": {},
   "source": [
    "# CHANGE the VALUE to HH:MM\n",
    "it should be in 0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# change format to 0000\n",
    "Airline_dataframe_new = Airline_dataframe.withColumn('SCHEDULED_TIME', lpad(col(\"SCHEDULED_TIME\"),4, '0'))\n",
    "Airline_dataframe_new.select(\"SCHEDULED_TIME\").show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dataframe_new = Airline_dataframe.withColumn('ELAPSED_TIME', lpad(col(\"ELAPSED_TIME\"),4, '0'))\n",
    "Airline_dataframe_new.select(\"ELAPSED_TIME\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc21c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_dataframe_new = Airline_dataframe.withColumn('AIR_TIME', lpad(col(\"AIR_TIME\"),4, '0'))\n",
    "Airline_dataframe_new.select(\"AIR_TIME\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format to \"HH:MM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37340abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "Airline_dataframe_new = Airline_dataframe_new.withColumn(\"AIR_TIME\", date_format(col(\"AIR_TIME\"), \"HH:mm\"))\n",
    "Airline_dataframe_new.select(\"AIR_TIME\").show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "Airline_dataframe_new = Airline_dataframe_new.withColumn(\"AIR_TIME\", concat(substring(col(\"AIR_TIME\"),1,2), \":\", substring(col(\"AIR_TIME\"),3,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3dc9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09102f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77189e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Airline_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b667714",
   "metadata": {},
   "source": [
    "# the most busy Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31297c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Count the number of flights per airport\n",
    "flights_per_airport = Airline_dataframe.groupBy(\"ORIGIN_AIRPORT\").agg(count(\"*\").alias(\"FLIGHT_COUNT\"))\n",
    "\n",
    "# Sort the airports by flight count in descending order\n",
    "flights_per_airport = flights_per_airport.orderBy(col(\"FLIGHT_COUNT\").desc())\n",
    "\n",
    "# Get the airport with the highest flight count\n",
    "most_busy_airport = flights_per_airport.first()[\"ORIGIN_AIRPORT\"]\n",
    "\n",
    "print(f\"The most Busy Airport is /n /n : {most_busy_airport}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a1b36",
   "metadata": {},
   "source": [
    "# mode value from CANCELLATION_REASON column and fill the null values with mode value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Compute the mode for each column\n",
    "Airline_dataframe_null_counts = Airline_dataframe.select(*(mean(col(c).isNull().cast(\"string\")).alias(c) for c in Airline_dataframe.columns))\n",
    "\n",
    "\n",
    "# Fill null values with the mode value\n",
    "#Airline_dataframe = Airline_dataframe.na.fill(Airline_dataframe_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions\n",
    "\n",
    "# Calculate the mode value\n",
    "mode_value = Airline_dataframe.select((col(\"CANCELLATION_REASON\")))\n",
    "mode_value = mode_value.collect()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d1add",
   "metadata": {},
   "source": [
    "# Destination with maximum departure delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62149c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group by destination AIRPORT and compute the maximum departure delay\n",
    "max_departure_delay = Airline_dataframe.groupBy(\"DESTINATION_AIRPORT\").agg(max(\"DEPARTURE_DELAY\").alias(\"MAX_DEPARTURE_DELAY\"))\n",
    "\n",
    "# Sort the DataFrame by maximum departure delay in descending order\n",
    "max_departure_delay = max_departure_delay.sort(max_departure_delay[\"MAX_DEPARTURE_DELAY\"].desc())\n",
    "\n",
    "\n",
    "max_departure_delay.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeab47a",
   "metadata": {},
   "source": [
    "# Destination with minimum departure delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef14c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "# Group by destination Airport and compute the minimum departure delay\n",
    "min_departure_delay = Airline_dataframe.groupBy(\"DESTINATION_AIRPORT\").agg(min(\"DEPARTURE_DELAY\").alias(\"MIN_DEPARTURE_DELAY\"))\n",
    "\n",
    "# Sort the DataFrame by minimum departure delay in ascending order\n",
    "min_departure_delay = min_departure_delay.sort(min_departure_delay[\"MIN_DEPARTURE_DELAY\"])\n",
    "\n",
    "# Display the destination airport with the minimum departure delay\n",
    "min_departure_delay.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a07aa0",
   "metadata": {},
   "source": [
    "#  max & min Delay calculations at the origin locations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay_with_origin = Airline_dataframe.groupBy(\"ORIGIN_AIRPORT\").agg(max(\"DEPARTURE_DELAY\").alias(\"MAX_DEPARTURE_DELAY\"))\n",
    "max_delay_with_origin = max_delay_with_origin.sort(max_delay_with_origin[\"MAX_DEPARTURE_DELAY\"].desc())\n",
    "max_delay_with_origin.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_delay_with_origin = Airline_dataframe.groupBy(\"ORIGIN_AIRPORT\").agg(min(\"DEPARTURE_DELAY\").alias(\"MIN_DEPARTURE_DELAY\"))\n",
    "min_delay_with_origin = min_delay_with_origin.sort(min_delay_with_origin[\"MIN_DEPARTURE_DELAY\"])\n",
    "min_delay_with_origin.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Airline_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43ff20",
   "metadata": {},
   "source": [
    "# Month during which Maximum Delays observed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "Max_delay_during_Month = demo.groupBy(\"MONTH\").agg(max(\"AIRLINE_DELAY\").alias(\"MAX_MONTH_DELAY\"))\n",
    "Max_delay_during_Month = Max_delay_during_Month.sort(Max_delay_during_Month[\"MAX_MONTH_DELAY\"].desc())\n",
    "Max_delay_during_Month.show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "Min_delay_during_Month = demo.groupBy(\"MONTH\").agg(min(\"AIRLINE_DELAY\").alias(\"MIN_MONTH_DELAY\"))\n",
    "Min_delay_during_Month = Min_delay_during_Month.sort(Min_delay_during_Month[\"MIN_MONTH_DELAY\"])\n",
    "Min_delay_during_Month.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bc7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d054c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcd511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bab88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef13e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9f699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338732ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "46a0df6f3d1db53ad22c67f58dc17d9da1cf5b39fcd4bfe6dddbb67b90845879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
